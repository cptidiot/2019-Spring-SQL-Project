{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('freshdirect.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out unique zip code in original dataset that have more than 20 customers\n",
    "temp = df.groupby('ZIP_CODE').count()['CUSTOMER_ID'].sort_values(ascending = False)\n",
    "temp1 = temp[temp>20].reset_index()\n",
    "\n",
    "df_zipcode = pd.DataFrame({'zip':temp1.ZIP_CODE})\n",
    "\n",
    "# read popluation data\n",
    "df_pop = pd.read_csv('zipcode_population.csv')\n",
    "\n",
    "# subset info we need\n",
    "df_pop = df_pop[['zip','city','population','density']]\n",
    "\n",
    "# merge popluation info to original dataset\n",
    "df_zip = pd.merge(df_zipcode, df_pop, on=['zip'], how='inner')\n",
    "\n",
    "# read income data\n",
    "df_income = pd.read_csv('zipcode_income.csv')\n",
    "\n",
    "# subset average total income\n",
    "df_income = df_income[['ZIPCODE','Avg total income']]\n",
    "\n",
    "# rename ZIPCODE to zip in df_income\n",
    "df_income.rename(columns={'ZIPCODE':'zip'}, inplace=True)\n",
    "\n",
    "# merge income to updated df\n",
    "df_zip = pd.merge(df_zip, df_income, on=['zip'], how='inner')\n",
    "\n",
    "df_zip.rename(columns={'Avg total income':'avg_total_income'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using city name to be dma name\n",
    "city = pd.DataFrame({'dma_name':df_zip.zip.unique()})\n",
    "\n",
    "# create unique dma id\n",
    "dma_id = []\n",
    "for i in range(0,len(city)):\n",
    "    dma_id.append('d' + str(i))\n",
    "dma_id = pd.DataFrame({'dma_id':dma_id})\n",
    "\n",
    "# import marketing_cost data\n",
    "marketing_cost = pd.DataFrame({'marketing_cost':np.random.randint(50000,200000,len(city))})\n",
    "\n",
    "# populate df with market source\n",
    "marketing_cost['method_of_marketing'] = np.random.choice(['digital ads', 'magazine', 'newspaper','TV','Mail'], marketing_cost.shape[0])\n",
    "\n",
    "# combine all data together\n",
    "df_dma = pd.concat([dma_id,city,marketing_cost], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquire_id = pd.DataFrame({'acquire_id':['a1','a2','a3','a4','a5','a6']})\n",
    "acquire_name = pd.DataFrame({'acquire_name':['online ads','referral','street campaign','holiday campaign','subway ads','email ads']})\n",
    "acqurie_cost = pd.DataFrame({'average_cost':[25,15,10,10,5,4]})\n",
    "\n",
    "df_acquire = pd.concat([acquire_id,acquire_name,acqurie_cost],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:5096: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# subset columns for customer table\n",
    "freq_df = df[df.ZIP_CODE.isin(temp1.ZIP_CODE)]\n",
    "df_customer = freq_df[['AGE',' INCOME ','GENDER','ZIP_CODE','ACQUIRED_DATE','LOYALTY_SEGMENT']]\n",
    "\n",
    "# process NA\n",
    "df_customer.AGE = df_customer.AGE.fillna(value = int(df_customer.AGE.mean()))\n",
    "df_customer[' INCOME '] = df_customer[' INCOME '].fillna (value = int( df_customer[' INCOME '].mean() ))\n",
    "#df_customer.GENDER = df_customer.GENDER.fillna(value = 'F')\n",
    "\n",
    "# process negative income\n",
    "df_customer[' INCOME '].loc[df_customer[' INCOME ']<0] = 50000\n",
    "\n",
    "# assign acquire source to each customer\n",
    "df_customer['ACQUIRE_SOURCE'] = np.random.choice(['a1','a2','a3','a4','a5','a6'],df_customer.shape[0])\n",
    "\n",
    "# assign dma for each customer\n",
    "df_customer = pd.merge(df_customer,df_zip,left_on = 'ZIP_CODE',right_on = 'zip',how = 'inner')\n",
    "\n",
    "df_customer.rename(columns = {'zip':'dma'},inplace = True)\n",
    "\n",
    "# drop unnecessary cols\n",
    "df_customer.drop(['population','density','avg_total_income','city'],axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "# add random customer name and address\n",
    "a = pd.read_csv('customer_1.csv')\n",
    "b = pd.read_csv('customer_2.csv')\n",
    "c = pd.read_csv('customer_3.csv')\n",
    "df_name_address = pd.concat([a,b,c],ignore_index=True)\n",
    "\n",
    "df_customer\n",
    "\n",
    "# merge name and address into customer\n",
    "df_customer = pd.concat([df_name_address,df_customer],axis = 1)\n",
    "df_customer.rename(columns = {' INCOME ':'income','AGE':'age','GENDER':'gender','ZIP_CODE':'zip_code','ACQUIRED_DATE':'acquired_date','LOYALTY_SEGMENT':'loyalty_segment','ACQUIRE_SOURCE':'acquire_source'},inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2562, 10)"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate unique review id\n",
    "r_id = []\n",
    "for i in range (1,20001):\n",
    "    r_id.append('r'+str(i))\n",
    "r_id = pd.DataFrame({'r_id':r_id})\n",
    "\n",
    "# generate quality review\n",
    "q_review = np.random.choice([2,2.5,3,3.5,4,4.5,5],20000)\n",
    "\n",
    "# generate delivery review\n",
    "d_review = np.random.choice([3,3.5,4,4.5,5],20000)\n",
    "\n",
    "# generate avg review\n",
    "avg_review = (q_review+d_review)/2\n",
    "\n",
    "# merge all data\n",
    "r_id['q_review'] = q_review\n",
    "r_id['d_review'] = d_review\n",
    "r_id['avg_review'] = avg_review\n",
    "\n",
    "df_review = r_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 2)\n",
      "(20000, 10)\n"
     ]
    }
   ],
   "source": [
    "# create order_id\n",
    "o_id = []\n",
    "for i in range(1,20001):\n",
    "    o_id.append('o'+str(i))\n",
    "o_id = pd.DataFrame({'order_id':o_id})\n",
    "\n",
    "#  select customer name and their addresses from customer table \n",
    "customer_name_temp = df_customer.customer_name\n",
    "customer_address_temp = df_customer.address\n",
    "\n",
    "customer_name = pd.DataFrame({'customer_name':np.random.choice(customer_name_temp,20000),'address':np.random.choice(customer_address_temp,20000)})\n",
    "\n",
    "\n",
    "# generate data for day_of_week and sales accrodingly with reasonable weights\n",
    "\n",
    "day_of_week = pd.DataFrame(columns = ['day_of_week','sales'])\n",
    "day_of_week.day_of_week = np.random.choice(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'],size = 20000,p = [0.05,0.05,0.1,0.1,0.3,0.3,0.1])\n",
    "\n",
    "def day_of_week_sales():\n",
    "    for i in range(len(day_of_week)):\n",
    "        if day_of_week.day_of_week[i] == 'Friday' or day_of_week.day_of_week[i] == 'Saturday':\n",
    "            day_of_week.sales[i] = np.random.randint(100,500)\n",
    "        else:\n",
    "            day_of_week.sales[i] = np.random.randint(30,250)\n",
    "    return day_of_week\n",
    "\n",
    "df_day_of_week = day_of_week_sales()\n",
    "\n",
    "# generate data for status\n",
    "status = pd.DataFrame({'status':np.random.choice(['Completed','Pending'],size = 20000,p = [0.7,0.3])})\n",
    "\n",
    "# generate random order date\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "def strTimeProp(start, end, format, prop):\n",
    "\n",
    "    stime = time.mktime(time.strptime(start, format))\n",
    "    etime = time.mktime(time.strptime(end, format))\n",
    "    ptime = stime + prop * (etime - stime)\n",
    "\n",
    "    return time.strftime(format, time.localtime(ptime))\n",
    "\n",
    "\n",
    "def randomDate(start, end, prop):\n",
    "    return strTimeProp(start, end, '%m/%d/%Y %I:%M %p', prop)\n",
    "\n",
    "\n",
    "temp = []\n",
    "for i in range(1,20001):\n",
    "    temp.append(randomDate(\"1/1/2018 1:30 PM\", \"3/31/2019 11:59 PM\", random.random()))\n",
    "\n",
    "date = pd.DataFrame({'date':temp})\n",
    "\n",
    "# generate data for total_dis\n",
    "total_dis = []\n",
    "for i in range(len(day_of_week)):\n",
    "    total_dis.append(day_of_week['sales'][i]*(1-round(random.uniform(0.7,1), 2)))\n",
    "total_discount = pd.DataFrame({'total_dis':total_dis})\n",
    "\n",
    "# generate delivery fee\n",
    "delivery_fee = pd.DataFrame({'delivery_fee':np.random.randint(0,10,20000)})\n",
    "\n",
    "# generate delivery pass\n",
    "delivery_pass = pd.DataFrame({'delivery_pass':np.random.choice(['True','False'],size = 20000,p=[0.7,0.3])})\n",
    "\n",
    "# combine all the columns\n",
    "\n",
    "df_order = pd.concat([o_id,customer_name,day_of_week,status,date,total_discount,delivery_fee,delivery_pass],axis=1)\n",
    "# combine review to df\n",
    "\n",
    "df_order = pd.concat([df_order,df_review['r_id']],axis = 1)\n",
    "\n",
    "df_order.rename(columns={'r_id':'review'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create unique id for discount table\n",
    "temp = []\n",
    "for i in range(1,31):\n",
    "    temp.append('dis'+str(i))\n",
    "d_id = pd.DataFrame({'discount_id':temp})\n",
    "\n",
    "# create discount rate\n",
    "rate = []\n",
    "for i in range(1,31):\n",
    "    rate.append(round(random.uniform(0.5,0.95), 2))\n",
    "d_rate = pd.DataFrame({'discount_rate':rate})\n",
    "\n",
    "# create description:\n",
    "desc = pd.DataFrame({'description':np.random.choice(['street campaign promotion','new customer promotion','Black Friday','subway ads promotion','newspaper ads promotion','Christmas Promotion'],size = 30, p = [0.1,0.3,0.2,0.2,0.1,0.1])})\n",
    "\n",
    "# create valid_from\n",
    "v_from = []\n",
    "for i in range(1,31):\n",
    "    v_from.append(randomDate(\"1/1/2018 4:00 PM\", \"6/1/2018 4:00 AM\", random.random()))\n",
    "v_from = pd.DataFrame({'valid_from':v_from})\n",
    "\n",
    "# create valid_to\n",
    "v_to = []\n",
    "for i in range(1,31):\n",
    "    v_to.append(randomDate(\"6/2/2018 1:30 PM\", \"6/5/2019 4:50 AM\", random.random()))\n",
    "v_to = pd.DataFrame({'valid_to':v_to})\n",
    "\n",
    "# combine them together\n",
    "df_discount = pd.concat([d_id,d_rate,desc,v_from,v_to],axis = 1)\n",
    "df_discount.rename(columns={'r_id':'review'}, inplace=True)\n",
    "\n",
    "\n",
    "df_discount = df_discount.append({'discount_id' : 'dis31' , 'discount_rate' : 1,'description' : 'No Discount','valid_from':'1900-01-01','valid_to':'9019-01-01'} , ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "# import items data to get sku information\n",
    "item = pd.read_csv('items.csv')\n",
    "\n",
    "# create temp df\n",
    "df_temp = pd.DataFrame()\n",
    "\n",
    "# randomly generate items for each order\n",
    "np.random.seed(111)\n",
    "for i in df_order.order_id:\n",
    "    df_temp = df_temp.append({'order_id':i,'items':item.sku.sample(np.random.randint(2,20)).tolist()},ignore_index=True)\n",
    "items = df_temp['items'].apply(pd.Series).stack().reset_index(level=1,drop = True).to_frame('items')\n",
    "\n",
    "\n",
    "df_temp = pd.merge(items, df_temp, left_index=True,right_index=True)\n",
    "\n",
    "del df_temp['items_y']\n",
    "df_temp = df_temp.rename(columns = {'items_x':'items'})\n",
    "\n",
    "       #df_temp['order_id']=df_temp.index\n",
    "df_temp = df_temp.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# randomly generate quantity for each items in each order\n",
    "temp = []\n",
    "for i in range(len(df_temp)):\n",
    "    temp.append(np.random.randint(1,10))\n",
    "quantity = pd.DataFrame({'quantity':temp})\n",
    "\n",
    "# randomly generate discount for each items\n",
    "quantity['discount'] = 'dis31'\n",
    "\n",
    "split = np.random.rand(len(discount)) < 0.3\n",
    "\n",
    "quantity.discount[split] = np.random.choice(df_discount.discount_id[0:30],len(split))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# combine all together\n",
    "df_order_items = pd.concat([df_temp,quantity],axis=1)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "duration = end-start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    210709.000000\n",
       "mean          5.001490\n",
       "std           2.584621\n",
       "min           1.000000\n",
       "25%           3.000000\n",
       "50%           5.000000\n",
       "75%           7.000000\n",
       "max           9.000000\n",
       "Name: quantity, dtype: float64"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_order_items['quantity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully connected to database\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "conn_url = 'postgresql://postgres:drv2i9es@s19db.apan5310.com:50208/test'\n",
    "engine = create_engine(conn_url)\n",
    "conn = engine.connect()\n",
    "print('successfully connected to database')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "conn_url = 'postgresql://marshall:@localhost:5432/marshall'\n",
    "engine = create_engine(conn_url)\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully dropped tables\n"
     ]
    }
   ],
   "source": [
    "conn.execute('drop table if exists review cascade')\n",
    "\n",
    "conn.execute('drop table if exists order_items cascade')\n",
    "\n",
    "conn.execute('drop table if exists orders cascade')\n",
    "\n",
    "conn.execute('drop table if exists dma cascade')\n",
    "\n",
    "conn.execute('drop table if exists customer cascade')\n",
    "\n",
    "conn.execute('drop table if exists acquire_source cascade')\n",
    "\n",
    "conn.execute('drop table if exists zipcode cascade')\n",
    "\n",
    "conn.execute('drop table if exists discount cascade')\n",
    "\n",
    "print('successfully dropped tables')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully created tables\n"
     ]
    }
   ],
   "source": [
    "conn. execute(\"\"\" \n",
    "\n",
    "create table dma(\n",
    " dma_id                 text,\n",
    " dma_name               text,\n",
    " marketing_cost         int,\n",
    " method_of_marketing    text,\n",
    " primary key (dma_name));\n",
    "\n",
    "\n",
    "create table zipcode(\n",
    " zip                   bigint,\n",
    " city                  text,\n",
    " population            int,\n",
    " density               numeric,\n",
    " avg_total_income      numeric,\n",
    " \n",
    " primary key (zip));\n",
    "     \n",
    "  \n",
    "create table acquire_source(\n",
    " acquire_id                   text,\n",
    " acquire_name                 text,\n",
    " average_cost                 int,\n",
    " primary key (acquire_id));\n",
    " \n",
    " \n",
    "   \n",
    "create table customer(\n",
    " customer_name           varchar(200),\n",
    " address                 text,\n",
    " age                     numeric,\n",
    " income                  numeric,\n",
    " gender                  text,\n",
    " zip_code                bigint,\n",
    " acquired_date           timestamp,\n",
    " loyalty_segment         text,\n",
    " acquire_source          text,\n",
    " dma                     text,\n",
    " primary key (customer_name),\n",
    " foreign key (dma) references dma(dma_name),\n",
    " foreign key (zip_code) references zipcode(zip),\n",
    " foreign key (acquire_source) references acquire_source(acquire_id));\n",
    " \n",
    " \n",
    " create table review(\n",
    " r_id            text,\n",
    " q_review        numeric,\n",
    " d_review        numeric,\n",
    " avg_review      numeric,\n",
    " primary key (r_id)) ;\n",
    " \n",
    " \n",
    " create table orders(\n",
    " order_id            text,\n",
    " customer_name       varchar(200),\n",
    " address             text,\n",
    " day_of_week         text,\n",
    " sales               int,\n",
    " status              text,\n",
    " date                timestamp,\n",
    " total_dis           numeric,\n",
    " delivery_fee        int,\n",
    " delivery_pass       text,\n",
    " review              text,\n",
    " primary key (order_id),\n",
    " foreign key (customer_name) references customer(customer_name),\n",
    " foreign key (review) references review(r_id));\n",
    " \n",
    " \n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "create table discount(\n",
    " discount_id    text,\n",
    " discount_rate   numeric,\n",
    " description    text,\n",
    " valid_from    text,\n",
    " valid_to    text,\n",
    " primary key (discount_id));\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "create table order_items(\n",
    " items        text,\n",
    " order_id     text,\n",
    " quantity     int,\n",
    " discount     text,\n",
    " primary key (items,order_id),\n",
    " foreign key (discount) references discount(discount_id),\n",
    " foreign key (items) references items_df(sku),\n",
    " \n",
    " foreign key (order_id) references orders(order_id) );\n",
    "     \n",
    "\"\"\")\n",
    "\n",
    "print('successfully created tables')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data into CVS for faster speed in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order is uploaded successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_order.to_csv('orders',index = False)\n",
    "print('order is uploaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipcode is uploaded successfully\n",
      "acquire is uploaded successfully\n",
      "customer is uploaded successfully\n",
      "review is uploaded successfully\n",
      "order is uploaded successfully\n",
      "discount is uploaded successfully\n",
      "order_times is uploaded successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48787379264831543"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = time.time()\n",
    "df_dma.to_csv('dma',index = False)\n",
    "\n",
    "\n",
    "df_zip.to_csv('zipcode',index = False)\n",
    "print('zipcode is uploaded successfully')\n",
    "\n",
    "\n",
    "df_acquire.to_csv('acquire_source',index = False)\n",
    "print('acquire is uploaded successfully')\n",
    "\n",
    "\n",
    "df_customer.to_csv('customer',index = False)\n",
    "print('customer is uploaded successfully')\n",
    "\n",
    "\n",
    "df_review.to_csv('review',index = False)\n",
    "print('review is uploaded successfully')\n",
    "\n",
    "\n",
    "df_order.to_csv('orders',index = False)\n",
    "print('order is uploaded successfully')\n",
    "\n",
    "\n",
    "\n",
    "df_discount.to_csv('discount',index = False)\n",
    "print('discount is uploaded successfully')\n",
    "\n",
    "\n",
    "df_order_items.to_csv('order_items',index = False)\n",
    "print('order_times is uploaded successfully')\n",
    "\n",
    "\n",
    "e = time.time()\n",
    "e-s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# populate database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dma is uploaded successfully\n",
      "zipcode is uploaded successfully\n",
      "acquire is uploaded successfully\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "(psycopg2.ProgrammingError) column \"AGE\" of relation \"customer\" does not exist\nLINE 1: INSERT INTO customer (\"AGE\", \" INCOME \", \"GENDER\", \"ZIP_CODE...\n                              ^\n [SQL: 'INSERT INTO customer (\"AGE\", \" INCOME \", \"GENDER\", \"ZIP_CODE\", \"ACQUIRED_DATE\", \"LOYALTY_SEGMENT\") VALUES (%(AGE)s, %( INCOME )s, %(GENDER)s, %(ZIP_CODE)s, %(ACQUIRED_DATE)s, %(LOYALTY_SEGMENT)s)'] [parameters: ({'AGE': 28.0, ' INCOME ': 40000.0, 'GENDER': 'F', 'ZIP_CODE': 11101, 'ACQUIRED_DATE': datetime.datetime(2012, 1, 7, 0, 0), 'LOYALTY_SEGMENT': '2. Bi-Weekly Shoppers'}, {'AGE': 42.0, ' INCOME ': 150000.0, 'GENDER': 'M', 'ZIP_CODE': 10012, 'ACQUIRED_DATE': datetime.datetime(2004, 3, 30, 0, 0), 'LOYALTY_SEGMENT': '2. Bi-Weekly Shoppers'}, {'AGE': 28.0, ' INCOME ': 150000.0, 'GENDER': 'F', 'ZIP_CODE': 10010, 'ACQUIRED_DATE': datetime.datetime(2012, 1, 8, 0, 0), 'LOYALTY_SEGMENT': '4. Once a Month Shoppers'}, {'AGE': 43.0, ' INCOME ': 50000.0, 'GENDER': 'M', 'ZIP_CODE': 10027, 'ACQUIRED_DATE': datetime.datetime(2006, 3, 26, 0, 0), 'LOYALTY_SEGMENT': '4. Once a Month Shoppers'}, {'AGE': None, ' INCOME ': None, 'GENDER': None, 'ZIP_CODE': 10021, 'ACQUIRED_DATE': datetime.datetime(2004, 3, 30, 0, 0), 'LOYALTY_SEGMENT': '1. Weekly Shoppers'}, {'AGE': None, ' INCOME ': None, 'GENDER': None, 'ZIP_CODE': 10010, 'ACQUIRED_DATE': datetime.datetime(2012, 2, 5, 0, 0), 'LOYALTY_SEGMENT': '2. Bi-Weekly Shoppers'}, {'AGE': None, ' INCOME ': None, 'GENDER': None, 'ZIP_CODE': 11231, 'ACQUIRED_DATE': datetime.datetime(2012, 1, 8, 0, 0), 'LOYALTY_SEGMENT': '3. Every Three Week Shoppers'}, {'AGE': 63.0, ' INCOME ': 300000.0, 'GENDER': 'F', 'ZIP_CODE': 10128, 'ACQUIRED_DATE': datetime.datetime(2012, 1, 7, 0, 0), 'LOYALTY_SEGMENT': '1. Weekly Shoppers'}  ... displaying 10 of 2562 total bound parameter sets ...  {'AGE': 63.0, ' INCOME ': 300000.0, 'GENDER': 'F', 'ZIP_CODE': 10021, 'ACQUIRED_DATE': datetime.datetime(2003, 1, 12, 0, 0), 'LOYALTY_SEGMENT': '1. Weekly Shoppers'}, {'AGE': 2.0, ' INCOME ': -1.0, 'GENDER': None, 'ZIP_CODE': 10003, 'ACQUIRED_DATE': datetime.datetime(2015, 11, 17, 0, 0), 'LOYALTY_SEGMENT': '5. Infrequent Shoppers'})] (Background on this error at: http://sqlalche.me/e/f405)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1169\u001b[0m                         \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m                         context)\n\u001b[0m\u001b[1;32m   1171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py\u001b[0m in \u001b[0;36mdo_executemany\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m             \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProgrammingError\u001b[0m: column \"AGE\" of relation \"customer\" does not exist\nLINE 1: INSERT INTO customer (\"AGE\", \" INCOME \", \"GENDER\", \"ZIP_CODE...\n                              ^\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-664-f50335a60d93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf_customer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'customer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mif_exists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'append'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'customer is uploaded successfully'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m   2530\u001b[0m         sql.to_sql(self, name, con, schema=schema, if_exists=if_exists,\n\u001b[1;32m   2531\u001b[0m                    \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2532\u001b[0;31m                    dtype=dtype, method=method)\n\u001b[0m\u001b[1;32m   2533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2534\u001b[0m     def to_pickle(self, path, compression='infer',\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m    458\u001b[0m     pandas_sql.to_sql(frame, name, if_exists=if_exists, index=index,\n\u001b[1;32m    459\u001b[0m                       \u001b[0mindex_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m                       chunksize=chunksize, dtype=dtype, method=method)\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m   1172\u001b[0m                          schema=schema, dtype=dtype)\n\u001b[1;32m   1173\u001b[0m         \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0;31m# check for potentially case sensitivity issues (GH7815)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, chunksize, method)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m                 \u001b[0mchunk_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m                 \u001b[0mexec_insert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     def _query_iterator(self, result, chunksize, columns, coerce_float=True,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36m_execute_insert\u001b[0;34m(self, conn, keys, data_iter)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \"\"\"\n\u001b[1;32m    598\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_insert_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, object, *multiparams, **params)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectNotExecutableError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/sql/elements.py\u001b[0m in \u001b[0;36m_execute_on_connection\u001b[0;34m(self, connection, multiparams, params)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_on_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_execution\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_clauseelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectNotExecutableError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_clauseelement\u001b[0;34m(self, elem, multiparams, params)\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0mcompiled_sql\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0mdistilled_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m             \u001b[0mcompiled_sql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistilled_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m         )\n\u001b[1;32m   1062\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1198\u001b[0m                 \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m                 \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m                 context)\n\u001b[0m\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 util.raise_from_cause(\n\u001b[1;32m   1412\u001b[0m                     \u001b[0msqlalchemy_exception\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m                     \u001b[0mexc_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m                 )\n\u001b[1;32m   1415\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/util/compat.py\u001b[0m in \u001b[0;36mraise_from_cause\u001b[0;34m(exception, exc_info)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mcause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexc_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexception\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc_tb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpy3k\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/util/compat.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb, cause)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1168\u001b[0m                         \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m                         \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m                         context)\n\u001b[0m\u001b[1;32m   1171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdialect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py\u001b[0m in \u001b[0;36mdo_executemany\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0mextras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m             \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoized_instancemethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProgrammingError\u001b[0m: (psycopg2.ProgrammingError) column \"AGE\" of relation \"customer\" does not exist\nLINE 1: INSERT INTO customer (\"AGE\", \" INCOME \", \"GENDER\", \"ZIP_CODE...\n                              ^\n [SQL: 'INSERT INTO customer (\"AGE\", \" INCOME \", \"GENDER\", \"ZIP_CODE\", \"ACQUIRED_DATE\", \"LOYALTY_SEGMENT\") VALUES (%(AGE)s, %( INCOME )s, %(GENDER)s, %(ZIP_CODE)s, %(ACQUIRED_DATE)s, %(LOYALTY_SEGMENT)s)'] [parameters: ({'AGE': 28.0, ' INCOME ': 40000.0, 'GENDER': 'F', 'ZIP_CODE': 11101, 'ACQUIRED_DATE': datetime.datetime(2012, 1, 7, 0, 0), 'LOYALTY_SEGMENT': '2. Bi-Weekly Shoppers'}, {'AGE': 42.0, ' INCOME ': 150000.0, 'GENDER': 'M', 'ZIP_CODE': 10012, 'ACQUIRED_DATE': datetime.datetime(2004, 3, 30, 0, 0), 'LOYALTY_SEGMENT': '2. Bi-Weekly Shoppers'}, {'AGE': 28.0, ' INCOME ': 150000.0, 'GENDER': 'F', 'ZIP_CODE': 10010, 'ACQUIRED_DATE': datetime.datetime(2012, 1, 8, 0, 0), 'LOYALTY_SEGMENT': '4. Once a Month Shoppers'}, {'AGE': 43.0, ' INCOME ': 50000.0, 'GENDER': 'M', 'ZIP_CODE': 10027, 'ACQUIRED_DATE': datetime.datetime(2006, 3, 26, 0, 0), 'LOYALTY_SEGMENT': '4. Once a Month Shoppers'}, {'AGE': None, ' INCOME ': None, 'GENDER': None, 'ZIP_CODE': 10021, 'ACQUIRED_DATE': datetime.datetime(2004, 3, 30, 0, 0), 'LOYALTY_SEGMENT': '1. Weekly Shoppers'}, {'AGE': None, ' INCOME ': None, 'GENDER': None, 'ZIP_CODE': 10010, 'ACQUIRED_DATE': datetime.datetime(2012, 2, 5, 0, 0), 'LOYALTY_SEGMENT': '2. Bi-Weekly Shoppers'}, {'AGE': None, ' INCOME ': None, 'GENDER': None, 'ZIP_CODE': 11231, 'ACQUIRED_DATE': datetime.datetime(2012, 1, 8, 0, 0), 'LOYALTY_SEGMENT': '3. Every Three Week Shoppers'}, {'AGE': 63.0, ' INCOME ': 300000.0, 'GENDER': 'F', 'ZIP_CODE': 10128, 'ACQUIRED_DATE': datetime.datetime(2012, 1, 7, 0, 0), 'LOYALTY_SEGMENT': '1. Weekly Shoppers'}  ... displaying 10 of 2562 total bound parameter sets ...  {'AGE': 63.0, ' INCOME ': 300000.0, 'GENDER': 'F', 'ZIP_CODE': 10021, 'ACQUIRED_DATE': datetime.datetime(2003, 1, 12, 0, 0), 'LOYALTY_SEGMENT': '1. Weekly Shoppers'}, {'AGE': 2.0, ' INCOME ': -1.0, 'GENDER': None, 'ZIP_CODE': 10003, 'ACQUIRED_DATE': datetime.datetime(2015, 11, 17, 0, 0), 'LOYALTY_SEGMENT': '5. Infrequent Shoppers'})] (Background on this error at: http://sqlalche.me/e/f405)"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "df_dma.to_sql('dma',engine,if_exists = 'append',index = False)\n",
    "print('dma is uploaded successfully')\n",
    "\n",
    "df_zip.to_sql('zipcode',engine,if_exists = 'append',index = False)\n",
    "print('zipcode is uploaded successfully')\n",
    "\n",
    "\n",
    "df_acquire.to_sql('acquire_source',engine,if_exists = 'append',index = False)\n",
    "print('acquire is uploaded successfully')\n",
    "\n",
    "\n",
    "df_customer.to_sql('customer',engine,if_exists = 'append',index = False)\n",
    "print('customer is uploaded successfully')\n",
    "\n",
    "\n",
    "df_review.to_sql('review',engine,if_exists = 'append',index = False)\n",
    "print('review is uploaded successfully')\n",
    "\n",
    "\n",
    "df_order.to_sql('orders',engine,if_exists = 'append',index = False)\n",
    "print('order is uploaded successfully')\n",
    "\n",
    "\n",
    "\n",
    "df_discount.to_sql('discount',engine,if_exists = 'append',index = False)\n",
    "print('discount is uploaded successfully')\n",
    "\n",
    "\n",
    "df_order_items.to_sql('order_items',engine,if_exists = 'append',index = False)\n",
    "print('order_times is uploaded successfully')\n",
    "\n",
    "\n",
    "e = time.time()\n",
    "e-s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# match zip_code with the coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = pd.read_csv('coordinate_original.csv')\n",
    "\n",
    "zip_co = pd.merge(df_zip,co,left_on = 'zip',right_on = 'ZIP',how = 'inner' )\n",
    "\n",
    "zip_co.drop(['population','density','avg_total_income','city','ZIP'],axis = 1, inplace = True)\n",
    "\n",
    "zip_co.rename(columns = {'LAT':'lat','LNG':'lon'},inplace = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
